# ONNX Runtime accuracy testing tool
This tool measures the accuracy of a set of models on a given execution provider. The accuracy is computed by comparing with the expected results, which are either loaded from file or attained by running the model with the CPU execution provider.

## Build instructions on Windows
Run the following commands in a terminal to generate a Visual Studio project and compile the tool. Make sure to specify the location of your ONNX Runtime installation. You can either [download an ONNX Runtime release package](https://github.com/microsoft/onnxruntime/releases/) or you can [build ONNX Runtime from source](https://www.onnxruntime.ai/docs/build/).

```shell
$ build.bat 'C:\onnxruntime'
```

Run the following command to open the solution file with Visual Studio.

```shell
$ devenv .\build\onnxruntime_accuracy_test.sln
```

Alternatively, you can directly run the executable from the terminal:

```shell
.\build\Release\accuracy_test.exe --help
```

### Building with QNN execution provider
To test model accuracy with the QNN execution provider, provide the path to location of your Qualcomm AI Engine Direct SDK (QNN SDK).
The QNN SDK can be downloaded from https://qpm.qualcomm.com/main/tools/details/qualcomm_ai_engine_direct.
Providing the QNN SDK path will ensure that the appropriate QNN SDK dynamic libraries (e.g., QnnHtp.dll) are automatically copied to the build directory.

```shell
$ build.bat 'C:\onnxruntime' 'C:\Qualcomm\AIStack\QNN\2.17.0.231124'
```


Note that you can also provide a [NuGet package](https://www.nuget.org/packages/Microsoft.ML.OnnxRuntime.QNN) as the first argument to `build.bat`:

```shell
$ build.bat '.\microsoft.ml.onnxruntime.qnn.1.16.0.nupkg' 'C:\Qualcomm\AIStack\QNN\2.17.0.231124'
```

## Setup test models and inputs
This tool expects all models and input files to be arranged in a specific directory structure.

```
models/
 |
 +--> resnet/
 |      |
 |      +--> model.onnx
 |      +--> model.qdq.onnx (quantized model only required for certains EPs like QNN)
 |      |
 |      +--> test_data_set_0/
 |      |        |
 |      |        +--> input_0.raw
 |      |        +--> input_1.raw
 |      |        +--> output_0.raw (optional, can be generated by tool)
 |      |        +--> output_1.raw (optional, can be generated by tool)
 |      +--> test_data_set_1/
 |      |
 |      +--> test_data_set_2/
 |
 +--> mobilenet/
        |
        +--> model.onnx
        +--> model.qdq.onnx
        |
        +--> test_data_set_0/
        +--> test_data_set_1/
```

- All ONNX models must be named either `model.onnx` or `model.qdq.onnx`.
  - The `model.qdq.onnx` file is only necessary for execution providers that run quantized models (e.g., QNN).
  - If the expected output files are not provided, the expected outputs will be obtained by running `model.onnx` on the CPU execution provider.
  - Both `model.qdq.onnx` and `model.onnx` must have the same input and output signature (i.e., same names, shapes, types, and ordering).
- The dataset directories must be named `test_data_set_<index>/`, where `<index>` ranges from 0 to the number of dataset directories.
- The raw input files must be named `input_<index>.raw`, where `<index>` corresponds to the input's index in the ONNX model.
- The raw output files are not required if `model.onnx` is provided.
  - The raw output files must be named `output_<index>.raw`, where `<index>` corresponds to the output's index in the ONNX model.
  - The raw output files can be automatically generated by the tool by specifying the `-save_expected_outputs` (`-s`) command-line argument.

## Command-line options
```shell
.\accuracy_test --help

Usage: accuracy_test.exe [OPTIONS...] test_models_path

[OPTIONS]:
 -h/--help                        Print this help message and exit program
 -j/--num_threads num_threads     Number of threads to use for inference.
                                  Defaults to number of cores.
 -l/--load_expected_outputs       Load expected outputs from raw output_<index>.raw files
                                  Defaults to false.
 -s/--save_expected_outputs       Save outputs from baseline model on CPU EP to disk as
                                  output_<index>.raw files. Defaults to false.
 -e/--execution_provider ep [EP_ARGS]  The execution provider to test (e.g., qnn or cpu)
                                       Defaults to CPU execution provider running QDQ model.
 -o/--output_file path                 The output file into which to save accuracy results
 -a/--expected_accuracy_file path      The file containing expected accuracy results

[EP_ARGS]: Specify EP-specific runtime options as key value pairs.
  Example: -e <provider_name> "<key1>|<value1> <key2>|<value2>"
  [QNN only] [backend_path]: QNN backend path (e.g., 'C:\Path\QnnHtp.dll')
  [QNN only] [profiling_level]: QNN profiling level, options: 'basic', 'detailed',
                                default 'off'.
  [QNN only] [rpc_control_latency]: QNN rpc control latency. default to 10.
  [QNN only] [vtcm_mb]: QNN VTCM size in MB. default to 0 (not set).
  [QNN only] [htp_performance_mode]: QNN performance mode, options: 'burst', 'balanced',
             'default', 'high_performance', 'high_power_saver',
             'low_balanced', 'low_power_saver', 'power_saver',
             'sustained_high_performance'. Defaults to 'default'.
  [QNN only] [qnn_context_priority]: QNN context priority, options: 'low', 'normal',
             'normal_high', 'high'. Defaults to 'normal'.
  [QNN only] [qnn_saver_path]: QNN Saver backend path. e.g 'C:\Path\QnnSaver.dll'.
  [QNN only] [htp_graph_finalization_optimization_mode]: QNN graph finalization
             optimization mode, options: '0', '1', '2', '3'. Default is '0'.
```

## Usage examples
### Measure accuracy of QDQ model on CPU EP
- The expected outputs are generated by running the float32 `model.onnx` on CPU EP.
- Accuracy results (SNR) are dumped to stdout

```shell
$ .\accuracy_test -e cpu models

Accuracy Results:
=================
model_a/test_data_set_0,17.640392603599537
model_a/test_data_set_1,21.326599488217347
model_a/test_data_set_2,16.712691432087745
...
```

Use the `-o` command-line option to write the accuracy results to file.
```shell
$ .\accuracy_test -o results.csv -e cpu models

[INFO]: Saved accuracy results to results.csv
```

### Dump (and load) the expected outputs to disk
Use the `-s` command-line option to dump the expected outputs to disk (e.g., output_0.raw). The expected outputs are obtained by running `model.onnx` on the CPU EP regardless of the EP passed to the `-e` command-line option.
```shell
$ .\accuracy_test -s -e cpu models

Accuracy Results:
=================
model_a/test_data_set_0,17.640392603599537
...
```

Use the `-l` command-line option to load the expected outputs directly from `output_<index>.raw` files.
```shell
$ .\accuracy_test -l -e cpu models

Accuracy Results:
=================
model_a/test_data_set_0,17.640392603599537
...
```

### Measure accuracy of QDQ model on QNN EP and detect regressions
- The expected outputs are generated by running the float32 `model.onnx` on CPU EP.
- Accuracy results (SNR) are dumped to results_0.csv
- Note: can also use the `-s` or `-l` command-line options to save or load the expected outputs as demonstrated above.

```shell
$ .\accuracy_test -e qnn "backend_path|QnnHtp.dll" -o results_0.csv models

Accuracy Results:
=================
model_a/test_data_set_0,17.640392603599537
model_a/test_data_set_1,21.426599488217347
model_a/test_data_set_2,16.812691432087745
...
```

Use the `-a` command-line option to compare subsequent runs with previous accuracy results (e.g., results_0.csv). This can help detect accuracy regressions.

```shell
.\accuracy_test -a results_o.csv -e qnn "backend_path|QnnHtp.dll" models

Accuracy Results:
=================
model_a/test_data_set_0,16.640392603599537
...

Comparing accuracy with results_0.csv
===============================================
Checking if model_a/test_data_set_0 degraded ... FAILED
        Output 0 SNR decreased: expected 17.640392603599537, actual 16.640392603599537

Checking if model_a/test_data_set_1 degraded ... PASSED
Checking if model_a/test_data_set_2 degraded ... PASSED
Checking if model_a/test_data_set_3 degraded ... PASSED
...

[INFO]: 10/11 tests passed.
[INFO]: 1/11 tests failed.
```
