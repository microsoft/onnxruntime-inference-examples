{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
        "Licensed under the MIT License.\n",
        "\n",
        "# Inference Bert Model for High Performance with ONNX Runtime on AzureML #\n",
        "\n",
        "This tutorial takes a pre-trained BERT model, converts it to ONNX, and deploys the ONNX model with ONNX Runtime through AzureML.\n",
        "In the following sections, we are going to use the Bert model trained with Stanford Question Answering Dataset (SQuAD) dataset as an example. Bert SQuAD model is used in question answering scenarios, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\n",
        "\n",
        "## Contents\n",
        "\n",
        "**Prerequisites** to set up your Azure ML work environments\n",
        "\n",
        "**Obtain model and convert to ONNX**\n",
        "\n",
        "**Deploy Bert model using ONNX Runtime and AzureML**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "To run on AzureML, you need:\n",
        "* Azure subscription\n",
        "* Azure Machine Learning Workspace\n",
        "* the Azure Machine Learning SDK\n",
        "* the Azure CLI and the Azure Machine learning CLI extension (> version 2.2.2)\n",
        "\n",
        "You might also find the following resources useful:\n",
        "* Understand the [architecture and terms](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture) introduced by Azure Machine Learning\n",
        "* The [Azure Portal](https://portal.azure.com) allows you to track the status of your deployments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1649374486622
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (1.9.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from torch) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from torch) (3.10.0.2)\n",
            "Requirement already satisfied: transformers in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (4.5.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from transformers) (2021.8.28)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from transformers) (4.62.2)\n",
            "Requirement already satisfied: packaging in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: filelock in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: numpy>=1.17 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: requests in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from transformers) (2.26.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from sacremoses->transformers) (1.16.0)\n",
            "Requirement already satisfied: click in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from sacremoses->transformers) (8.0.1)\n",
            "Requirement already satisfied: joblib in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests->transformers) (3.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: azureml in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (0.2.7)\n",
            "Collecting azureml.core\n",
            "  Using cached azureml_core-1.40.0-py3-none-any.whl (2.7 MB)\n",
            "Requirement already satisfied: requests in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml) (2.26.0)\n",
            "Requirement already satisfied: python-dateutil in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml) (2.8.2)\n",
            "Requirement already satisfied: pandas in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml) (0.25.3)\n",
            "Requirement already satisfied: PyJWT<3.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (2.1.0)\n",
            "Requirement already satisfied: cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<37.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (3.4.8)\n",
            "Requirement already satisfied: backports.tempfile in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (1.0)\n",
            "Requirement already satisfied: azure-mgmt-resource<21.0.0,>=15.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (20.1.0)\n",
            "Requirement already satisfied: ndg-httpsclient<=0.5.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (0.5.1)\n",
            "Requirement already satisfied: azure-core<1.22 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (1.18.0)\n",
            "Requirement already satisfied: SecretStorage<4.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (3.3.1)\n",
            "Requirement already satisfied: jsonpickle<3.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (2.0.0)\n",
            "Requirement already satisfied: humanfriendly<11.0,>=4.7 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (9.2)\n",
            "Requirement already satisfied: msal-extensions<0.4,>=0.3.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (0.3.1)\n",
            "Requirement already satisfied: azure-mgmt-keyvault<10.0.0,>=0.40.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (9.1.0)\n",
            "Requirement already satisfied: azure-mgmt-containerregistry<9.0.0,>=8.2.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (8.2.0)\n",
            "Requirement already satisfied: azure-graphrbac<1.0.0,>=0.40.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (0.61.1)\n",
            "Requirement already satisfied: jmespath<1.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (0.10.0)\n",
            "Requirement already satisfied: knack~=0.9.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (0.9.0)\n",
            "Requirement already satisfied: pytz in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (2021.1)\n",
            "Requirement already satisfied: msal<2.0.0,>=1.15.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (1.17.0)\n",
            "Requirement already satisfied: contextlib2<22.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (21.6.0)\n",
            "Requirement already satisfied: azure-common<2.0.0,>=1.1.12 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (1.1.27)\n",
            "Requirement already satisfied: argcomplete<2.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (1.12.3)\n",
            "Requirement already satisfied: docker<6.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (5.0.2)\n",
            "Requirement already satisfied: adal<=1.2.7,>=1.2.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (1.2.7)\n",
            "Requirement already satisfied: azure-mgmt-storage<20.0.0,>=16.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (19.1.0)\n",
            "Requirement already satisfied: urllib3<=1.26.7,>=1.23 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (1.25.11)\n",
            "Requirement already satisfied: packaging<22.0,>=20.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (21.0)\n",
            "Requirement already satisfied: paramiko<3.0.0,>=2.0.8 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (2.7.2)\n",
            "Requirement already satisfied: pkginfo in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (1.7.1)\n",
            "Requirement already satisfied: pathspec<1.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (0.9.0)\n",
            "Requirement already satisfied: msrest<1.0.0,>=0.5.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (0.6.21)\n",
            "Requirement already satisfied: pyopenssl<22.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (20.0.1)\n",
            "Requirement already satisfied: azure-mgmt-authorization<1.0.0,>=0.40.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (0.61.0)\n",
            "Requirement already satisfied: msrestazure<=0.6.4,>=0.4.33 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml.core) (0.6.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests->azureml) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests->azureml) (3.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests->azureml) (2021.5.30)\n",
            "Requirement already satisfied: six>=1.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from python-dateutil->azureml) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from pandas->azureml) (1.18.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<37.0.0->azureml.core) (1.14.6)\n",
            "Requirement already satisfied: backports.weakref in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from backports.tempfile->azureml.core) (1.0.post1)\n",
            "Requirement already satisfied: azure-mgmt-core<2.0.0,>=1.3.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azure-mgmt-resource<21.0.0,>=15.0.0->azureml.core) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from ndg-httpsclient<=0.5.1->azureml.core) (0.4.8)\n",
            "Requirement already satisfied: jeepney>=0.6 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from SecretStorage<4.0.0->azureml.core) (0.7.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from jsonpickle<3.0.0->azureml.core) (4.8.1)\n",
            "Requirement already satisfied: portalocker<3,>=1.0; python_version >= \"3.5\" and platform_system != \"Windows\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from msal-extensions<0.4,>=0.3.0->azureml.core) (1.7.1)\n",
            "Requirement already satisfied: pygments in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from knack~=0.9.0->azureml.core) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from knack~=0.9.0->azureml.core) (5.4.1)\n",
            "Requirement already satisfied: tabulate in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from knack~=0.9.0->azureml.core) (0.8.9)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from docker<6.0.0->azureml.core) (1.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from packaging<22.0,>=20.0->azureml.core) (2.4.7)\n",
            "Requirement already satisfied: bcrypt>=3.1.3 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from paramiko<3.0.0,>=2.0.8->azureml.core) (3.2.0)\n",
            "Requirement already satisfied: pynacl>=1.0.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from paramiko<3.0.0,>=2.0.8->azureml.core) (1.4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from msrest<1.0.0,>=0.5.1->azureml.core) (1.3.0)\n",
            "Requirement already satisfied: isodate>=0.6.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from msrest<1.0.0,>=0.5.1->azureml.core) (0.6.0)\n",
            "Requirement already satisfied: pycparser in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from cffi>=1.12->cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<37.0.0->azureml.core) (2.20)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle<3.0.0->azureml.core) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle<3.0.0->azureml.core) (3.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests-oauthlib>=0.5.0->msrest<1.0.0,>=0.5.1->azureml.core) (3.1.1)\n",
            "\u001b[31mERROR: azureml-widgets 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-train-core 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-train-automl-runtime 1.34.0.post1 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-train-automl-client 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-tensorboard 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-telemetry 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-sdk 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-responsibleai 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-responsibleai 1.34.0 has requirement responsibleai==0.9.4, but you'll have responsibleai 0.7.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-pipeline-core 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-opendatasets 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-mlflow 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-interpret 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-defaults 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-datadrift 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-contrib-services 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-contrib-server 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-contrib-reinforcementlearning 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-contrib-pipeline-steps 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-contrib-notebook 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-contrib-fairness 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-contrib-dataset 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-cli-common 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-automl-dnn-nlp 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-accel-models 1.34.0 has requirement azureml-core~=1.34.0, but you'll have azureml-core 1.40.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: azureml.core\n",
            "Successfully installed azureml.core\n",
            "Requirement already satisfied: onnxruntime in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (1.8.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from onnxruntime) (1.18.5)\n",
            "Requirement already satisfied: flatbuffers in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from onnxruntime) (2.0)\n",
            "Requirement already satisfied: protobuf in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from onnxruntime) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from protobuf->onnxruntime) (1.16.0)\n",
            "Requirement already satisfied: matplotlib in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (3.2.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from matplotlib) (1.18.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from cycler>=0.10->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# To install dependencies directly run the following\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install azureml azureml.core\n",
        "!pip install onnxruntime\n",
        "!pip install matplotlib\n",
        "\n",
        "# To create a a Jupter kernel from your conda environment, run the following. replacing <kernel name> with your own name\n",
        "#   conda install -c anaconda ipykernel\n",
        "#   python -m ipykernel install --user --name=<kernel name>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Obtain and convert PyTorch model to ONNX format\n",
        "\n",
        "In the code below, we obtain a BERT model fine-tuned for question answering with the SQUAD dataset from HuggingFace.\n",
        "\n",
        "If you'd like to pre-train a BERT model from scratch, follow the instructions in\n",
        "[Pretraining of the BERT model](https://github.com/microsoft/AzureML-BERT/blob/master/pretrain/PyTorch/notebooks/BERT_Pretrain.ipynb). \n",
        "And if you would like to fine-tune the model with your own dataset, refer to  [AzureML Bert Eval Squad](https://github.com/microsoft/AzureML-BERT/blob/master/finetune/PyTorch/notebooks/BERT_Eval_SQUAD.ipynb)\n",
        "or [AzureML Bert Eval GLUE](https://github.com/microsoft/AzureML-BERT/blob/master/finetune/PyTorch/notebooks/BERT_Eval_GLUE.ipynb).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Define the tokenizer and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1649374487993
        },
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
        "model_path = \"./\" + model_name + \".onnx\"\n",
        "\n",
        "from transformers import BertTokenizer, BertForQuestionAnswering\n",
        "\n",
        "model = BertForQuestionAnswering.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1649374493959
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "max_seq_len = 1024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Sample input and question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1649374494183
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#question = \"What is a major importance of Southern California in relation to California and the United States?\"\n",
        "#context = \"Southern California, often abbreviated SoCal, is a geographic and cultural region that generally comprises California's southernmost 10 counties. The region is traditionally described as \\\"eight counties\\\", based on demographics and economic ties: Imperial, Los Angeles, Orange, Riverside, San Bernardino, San Diego, Santa Barbara, and Ventura. The more extensive 10-county definition, including Kern and San Luis Obispo counties, is also used based on historical political divisions. Southern California is a major economic center for the state of California and the United States.\"\n",
        "question = \"What is my name\"\n",
        "context = \"My name is Natalie and my friend's name is Jane\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Export the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1649374524671
        },
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "model_path = \"./\" + model_name + \".onnx\"\n",
        "\n",
        "# set the model to inference mode\n",
        "# It is important to call torch_model.eval() or torch_model.train(False) before exporting the model, \n",
        "# to turn the model to inference mode. This is required since operators like dropout or batchnorm \n",
        "# behave differently in inference and training mode.\n",
        "model.eval()\n",
        "\n",
        "# Generate dummy inputs to the model. Adjust if neccessary\n",
        "inputs = {\n",
        "        'input_ids':   torch.randint(32, [1, 32], dtype=torch.long), # list of numerical ids for the tokenized text\n",
        "        'attention_mask': torch.ones([1, 32], dtype=torch.long),     # dummy list of ones\n",
        "        'token_type_ids':  torch.ones([1, 32], dtype=torch.long)     # dummy list of ones\n",
        "    }\n",
        "\n",
        "symbolic_names = {0: 'batch_size', 1: 'max_seq_len'}\n",
        "torch.onnx.export(model,                                         # model being run\n",
        "                  (inputs['input_ids'],\n",
        "                   inputs['attention_mask'], \n",
        "                   inputs['token_type_ids']),                    # model input (or a tuple for multiple inputs)\n",
        "                  model_path,                                    # where to save the model (can be a file or file-like object)\n",
        "                  opset_version=11,                              # the ONNX version to export the model to\n",
        "                  do_constant_folding=True,                      # whether to execute constant folding for optimization\n",
        "                  input_names=['input_ids',\n",
        "                               'input_mask', \n",
        "                               'segment_ids'],                   # the model's input names\n",
        "                  output_names=['start_logits', \"end_logits\"],   # the model's output names\n",
        "                  dynamic_axes={'input_ids': symbolic_names,\n",
        "                                'input_mask' : symbolic_names,\n",
        "                                'segment_ids' : symbolic_names,\n",
        "                                'start_logits' : symbolic_names, \n",
        "                                'end_logits': symbolic_names})   # variable length axes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model_name' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[1;32m~\\Develop\\code\\microsoft\\onnxruntime-inference-examples\\python\\azureml\\export.py:4\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='file:///c%3A/Users/nakersha/Develop/code/microsoft/onnxruntime-inference-examples/python/azureml/export.py?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m----> <a href='file:///c%3A/Users/nakersha/Develop/code/microsoft/onnxruntime-inference-examples/python/azureml/export.py?line=3'>4</a>\u001b[0m model_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m model_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.onnx\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='file:///c%3A/Users/nakersha/Develop/code/microsoft/onnxruntime-inference-examples/python/azureml/export.py?line=5'>6</a>\u001b[0m \u001b[39m# set the model to inference mode\u001b[39;00m\n\u001b[0;32m      <a href='file:///c%3A/Users/nakersha/Develop/code/microsoft/onnxruntime-inference-examples/python/azureml/export.py?line=6'>7</a>\u001b[0m \u001b[39m# It is important to call torch_model.eval() or torch_model.train(False) before exporting the model, \u001b[39;00m\n\u001b[0;32m      <a href='file:///c%3A/Users/nakersha/Develop/code/microsoft/onnxruntime-inference-examples/python/azureml/export.py?line=7'>8</a>\u001b[0m \u001b[39m# to turn the model to inference mode. This is required since operators like dropout or batchnorm \u001b[39;00m\n\u001b[0;32m      <a href='file:///c%3A/Users/nakersha/Develop/code/microsoft/onnxruntime-inference-examples/python/azureml/export.py?line=8'>9</a>\u001b[0m \u001b[39m# behave differently in inference and training mode.\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/nakersha/Develop/code/microsoft/onnxruntime-inference-examples/python/azureml/export.py?line=9'>10</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n",
            "\u001b[1;31mNameError\u001b[0m: name 'model_name' is not defined"
          ]
        }
      ],
      "source": [
        "%run export.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Run the ONNX model with ONNX Runtime\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "The following code runs the ONNX model with ONNX Runtime. You can test it locally before deploying it to Azure Machine Learning.\n",
        "\n",
        "The `init()` function is called at startup, performing the one-off operations such as creating the tokenizer and the ONNX Runtime session.\n",
        "\n",
        "The `run()` function is called when we run the model using the Azure ML web service.\n",
        "Add neccessary `preprocess()` and `postprocess()` steps.\n",
        "\n",
        "For local testing and comparison, we also run the PyTorch model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting score.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile score.py\n",
        "import os\n",
        "import logging\n",
        "import json\n",
        "import numpy as np\n",
        "import onnxruntime\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "\n",
        "def preprocess(question, context):\n",
        "    print(\"Question:\", question)\n",
        "    print(\"Context: \", context)\n",
        "    encoded_input = tokenizer(question, context)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(encoded_input.input_ids)\n",
        "    print(tokens)\n",
        "    return (encoded_input.input_ids, encoded_input.attention_mask, encoded_input.token_type_ids, tokens)\n",
        "\n",
        "def postprocess(tokens, start, end):\n",
        "    print(\"Start:\", start)\n",
        "    print(\"End:\", end)\n",
        "    results = {}\n",
        "    answer_start = np.argmax(start)\n",
        "    answer_end = np.argmax(end)\n",
        "    print(\"Start: \", answer_start)\n",
        "    print(\"End: \", answer_end)\n",
        "    if answer_end >= answer_start:\n",
        "        answer = tokens[answer_start]\n",
        "        for i in range(answer_start+1, answer_end+1):\n",
        "            if tokens[i][0:2] == \"##\":\n",
        "                answer += tokens[i][2:]\n",
        "            else:\n",
        "                answer += \" \" + tokens[i]\n",
        "        results['answer'] = answer.capitalize()\n",
        "    else:\n",
        "        results['error'] = \"I am unable to find the answer to this question. Can you please ask another question?\"\n",
        "    return results\n",
        "\n",
        "def init():\n",
        "    global tokenizer, session, model\n",
        "\n",
        "    model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
        "    model = transformers.BertForQuestionAnswering.from_pretrained(model_name)\n",
        "\n",
        "    # use AZUREML_MODEL_DIR to get your deployed model(s). If multiple models are deployed, \n",
        "    # model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), '$MODEL_NAME/$VERSION/$MODEL_FILE_NAME')\n",
        "    model_dir = os.getenv('AZUREML_MODEL_DIR')\n",
        "    if model_dir == None:\n",
        "        model_dir = \"./\"\n",
        "    model_path = os.path.join(model_dir, model_name + \".onnx\")\n",
        "\n",
        "    # Create the tokenizer\n",
        "    tokenizer = transformers.BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Create an ONNX Runtime session to run the ONNX model\n",
        "    session = onnxruntime.InferenceSession(model_path, providers=[\"CPUExecutionProvider\"])  \n",
        "\n",
        "\n",
        "def run_pytorch(raw_data):\n",
        "    inputs = json.loads(raw_data)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    logging.info(\"Question:\", inputs[\"question\"])\n",
        "    logging.info(\"Context: \", inputs[\"context\"])\n",
        "\n",
        "    input_ids, input_mask, segment_ids, tokens = preprocess(inputs[\"question\"], inputs[\"context\"])\n",
        "\n",
        "    print(input_ids)\n",
        "\n",
        "    model_outputs = model(torch.tensor([input_ids]),  token_type_ids=torch.tensor([segment_ids]))\n",
        "\n",
        "    outputs = postprocess(tokens, model_outputs.start_logits.detach().numpy(), model_outputs.end_logits.detach().numpy())\n",
        "\n",
        "    print(outputs)\n",
        "\n",
        "\n",
        "def run(raw_data):\n",
        "    logging.info(\"Request received\")\n",
        "    inputs = json.loads(raw_data)\n",
        "\n",
        "    logging.info(inputs)\n",
        "\n",
        "    # Preprocess the question and context into tokenized ids\n",
        "    input_ids, input_mask, segment_ids, tokens = preprocess(inputs[\"question\"], inputs[\"context\"])\n",
        "\n",
        "    print(input_ids)\n",
        "    \n",
        "    # Format the inputs for ONNX Runtime\n",
        "    model_inputs = {\n",
        "        'input_ids':   [input_ids], \n",
        "        'input_mask':  [input_mask],\n",
        "        'segment_ids': [segment_ids]\n",
        "        }\n",
        "                  \n",
        "    outputs = session.run(['start_logits', 'end_logits'], model_inputs)\n",
        "    \n",
        "    # Post process the output of the model into an answer (or an error if the question could not be answered)\n",
        "    return postprocess(tokens, outputs[0], outputs[1])\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    init()\n",
        "\n",
        "    #input = \"{\\\"question\\\": \\\"What is my name?\\\", \\\"context\\\": \\\"My name is Natalie, and my sister's name is also Nathalie and my brother's name is dufas and my friend's name is Remy\\\"}\"\n",
        "\n",
        "    #input = \"{\\\"question\\\": \\\"What is Dolly Parton's middle name?\\\", \\\"context\\\": \\\"Dolly Rebecca Parton (born January 19, 1946) is an American singer-songwriter, actress, and businesswoman, known primarily for her work in country music. After achieving success as a songwriter for others, Parton made her album debut in 1967 with Hello, I'm Dolly, which led to success during the remainder of the 1960s (both as a solo artist and with a series of duet albums with Porter Wagoner), before her sales and chart peak came during the 1970s and continued into the 1980s. Parton's albums in the 1990s did not sell as well, but she achieved commercial success again in the new millennium and has released albums on various independent labels since 2000, including her own label, Dolly Records. She has sold more than 100 million records worldwide.\\\"}\"\n",
        "\n",
        "    input = \"{\\\"question\\\": \\\"What is Dolly Parton's middle name?\\\", \\\"context\\\": \\\"Dolly Rebecca Parton is an American singer-songwriter\\\"}\"\n",
        "\n",
        "    run_pytorch(input)\n",
        "    print(run(input))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is Dolly Parton's middle name?\n",
            "Context:  Dolly Rebecca Parton is an American singer-songwriter\n",
            "['[CLS]', 'what', 'is', 'dolly', 'part', '##on', \"'\", 's', 'middle', 'name', '?', '[SEP]', 'dolly', 'rebecca', 'part', '##on', 'is', 'an', 'american', 'singer', '-', 'songwriter', '[SEP]']\n",
            "[101, 2054, 2003, 19958, 2112, 2239, 1005, 1055, 2690, 2171, 1029, 102, 19958, 9423, 2112, 2239, 2003, 2019, 2137, 3220, 1011, 6009, 102]\n",
            "Start: [[-5.7373166 -2.0882578 -6.6572905 -4.736242  -7.208626  -8.217027\n",
            "  -8.110763  -7.1893578 -5.3911524 -6.7148128 -8.595815  -5.7372403\n",
            "   2.8533413  6.8337164 -4.656281  -5.8789706 -5.630715  -5.2006907\n",
            "  -4.8701143 -4.541009  -7.904859  -5.5882034 -5.737352 ]]\n",
            "End: [[-1.1449381  -0.75719965 -4.437096   -4.4248323  -6.008774   -2.813389\n",
            "  -5.418213   -4.610936   -4.350332   -4.110813   -5.120949   -1.144799\n",
            "   0.30320665  7.0895014  -3.892273    2.120804   -4.046403   -4.8648496\n",
            "  -3.4395008  -4.4262953  -5.8198347  -2.6823187  -1.144946  ]]\n",
            "Start:  13\n",
            "End:  13\n",
            "{'answer': 'Rebecca'}\n",
            "Question: What is Dolly Parton's middle name?\n",
            "Context:  Dolly Rebecca Parton is an American singer-songwriter\n",
            "['[CLS]', 'what', 'is', 'dolly', 'part', '##on', \"'\", 's', 'middle', 'name', '?', '[SEP]', 'dolly', 'rebecca', 'part', '##on', 'is', 'an', 'american', 'singer', '-', 'songwriter', '[SEP]']\n",
            "[101, 2054, 2003, 19958, 2112, 2239, 1005, 1055, 2690, 2171, 1029, 102, 19958, 9423, 2112, 2239, 2003, 2019, 2137, 3220, 1011, 6009, 102]\n",
            "Start: [[-5.7373185 -2.0882645 -6.6572933 -4.736243  -7.20863   -8.217031\n",
            "  -8.110762  -7.1893573 -5.391154  -6.714815  -8.595816  -5.7372437\n",
            "   2.8533506  6.833717  -4.65628   -5.8789687 -5.630716  -5.20069\n",
            "  -4.8701134 -4.5410085 -7.90486   -5.588201  -5.7373548]]\n",
            "End: [[-1.144938   -0.75720346 -4.4370937  -4.4248333  -6.0087748  -2.8133912\n",
            "  -5.4182143  -4.610935   -4.350332   -4.110814   -5.1209435  -1.1447994\n",
            "   0.3032107   7.0895042  -3.8922737   2.1208122  -4.046401   -4.8648477\n",
            "  -3.4395006  -4.426294   -5.8198338  -2.6823175  -1.1449456 ]]\n",
            "Start:  13\n",
            "End:  13\n",
            "{'answer': 'Rebecca'}\n"
          ]
        }
      ],
      "source": [
        "%run score.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy model with ONNX Runtime through AzureML\n",
        "\n",
        "Now that we have the ONNX model and the code to run it with ONNX Runtime, we can deploy it using Azure ML.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check your environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1649374558896
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformers version:  4.17.0\n",
            "Torch (ONNX exporter) version:  1.10.0\n",
            "Azure SDK version: 1.40.0\n",
            "ONNX Runtime version:  1.11.0\n"
          ]
        }
      ],
      "source": [
        "# Check core SDK version number\n",
        "import azureml.core\n",
        "import onnxruntime\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "print(\"Transformers version: \", transformers.__version__)\n",
        "torch_version = torch.__version__\n",
        "print(\"Torch (ONNX exporter) version: \", torch_version)\n",
        "print(\"Azure SDK version:\", azureml.core.VERSION)\n",
        "print(\"ONNX Runtime version: \", onnxruntime.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load your Azure ML workspace\n",
        "\n",
        "We begin by instantiating a workspace object from the existing workspace created earlier in the configuration notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1649374560631
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ort_training_dev\n",
            "australiaeast\n",
            "onnx_training\n"
          ]
        }
      ],
      "source": [
        "from azureml.core import Workspace\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "print(ws.name, ws.location, ws.resource_group, sep = '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Register your model with Azure ML\n",
        "\n",
        "Now we upload the model and register it in the workspace.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1649374588035
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./bert-large-uncased-whole-word-masking-finetuned-squad.onnx\n",
            "bert-large-uncased-whole-word-masking-finetuned-squad\n",
            "Registering model bert-large-uncased-whole-word-masking-finetuned-squad\n"
          ]
        }
      ],
      "source": [
        "from azureml.core.model import Model\n",
        "\n",
        "model = Model.register(model_path = model_path,                 # Name of the registered model in your workspace.\n",
        "                       model_name = model_name,            # Local ONNX model to upload and register as a model\n",
        "                       model_framework=Model.Framework.ONNX ,   # Framework used to create the model.\n",
        "                       model_framework_version=torch_version,   # Version of ONNX used to create the model.\n",
        "                       tags = {\"onnx\": \"demo\"},\n",
        "                       description = \"HuggingFace Bert model fine-tuned with SQuAd and exported from PyTorch\",\n",
        "                       workspace = ws)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Displaying your registered models\n",
        "\n",
        "You can list out all the models that you have registered in this workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1649374588649
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: hf-gpt2.onnx \tVersion: 1 \tDescription: ONNX version of base HuggingFace GPT-2 {}\n",
            "Name: hf-gpt2.pt \tVersion: 1 \tDescription: GPT-2 model saved from pre-trained HuggingFace {}\n",
            "Name: pytorch-hf-gpt-onnx-int8 \tVersion: 1 \tDescription: None {}\n",
            "Name: pytorch-hf-gpt2-wikitext103 \tVersion: 1 \tDescription: None {}\n",
            "Name: pt-ort-hf-gpt2-wt103-full \tVersion: 1 \tDescription: HuggingFace GPT-2 fine-tuned with PyTorch ORT using Wikitext103 {}\n",
            "Name: sample-densenet-onnx-model \tVersion: 1 \tDescription: None {}\n",
            "Name: bert-large-uncased-whole-word-masking-finetuned-squad \tVersion: 2 \tDescription: HuggingFace Bert model fine-tuned with SQuAd and exported from PyTorch {'onnx': 'demo'}\n"
          ]
        }
      ],
      "source": [
        "models = ws.models\n",
        "for name, m in models.items():\n",
        "    print(\"Name:\", name,\"\\tVersion:\", m.version, \"\\tDescription:\", m.description, m.tags)\n",
        "    \n",
        "#     # If you'd like to delete the models from workspace\n",
        "#     model_to_delete = Model(ws, name)\n",
        "#     model_to_delete.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy the model \n",
        "\n",
        "We are now going to deploy our ONNX model on Azure ML using ONNX Runtime.\n",
        "\n",
        "Firstly we will test the deployment using an Azure Container Instance, then deploy the model for production using an Azure ML endpoint.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy Model and Scoring code as an Azure ML endpoint\n",
        "\n",
        "Note that the endpoint interface of the Python SDK is in private preview. For this section, we will use the Azure ML CLI.\n",
        "\n",
        "There are three YML files in the `yml` folder:\n",
        "* `env.yml`: A conda environment specification, from which the execution environment of the endpoint will be generated\n",
        "* `endpoint.yml`: The endpoint specification, which simply contains the name of the endpoint and the authorization method\n",
        "* `deployment.yml`: The deployment specification, which contains specifications of the scoring code, model, and environment. You can create multiple deployments per endpoint, and route different amounts of traffic to the deployments. For this example, we will create only one deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check how to specify the proportion of traffic\n",
        "!az ml oneline-endpoint create --name question-answer-ort --file yml/endpoint.yml\n",
        "!az ml online-deployment create --endpoint-name question-answer-ort --name blue --file yml/deployment.yml "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test the endpoint\n",
        "az ml online-endpoint invoke --name question-answer-ort --request-file test-data.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Success!\n",
        "\n",
        "If you've made it this far, you've deployed a working endpoint that answers a question using an ONNX model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Measure the inference latency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean up Azure resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "f0528bd5f6ac7ac52612a9005b50b1458e2b7d1852103f3c7d5ad6e0d876b262"
    },
    "kernel_info": {
      "name": "gpu-kernel"
    },
    "kernelspec": {
      "display_name": "gpu-kernel",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
