{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX Runtime Sentiment Analysis with DistilBert model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you will learn the end-to-end steps from obtaining a HuggingFace model, converting to ONNX format, adding pre/post processing steps to the ONNX model using onnxruntime-extensions library and finally plug in and apply inference in a sample mobile android/ios app if applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to pip install `onnxruntime onnx onnxruntime_extensions transformers` as the necessary libraries.\n",
    "\n",
    "```\n",
    "    pip install onnx onnxruntime onnxruntime_extensions\n",
    "```\n",
    "```\n",
    "    pip install transformers\n",
    "```\n",
    "\n",
    "To work with Python in Jupyter Notebooks, you must activate an [Anaconda](https://www.anaconda.com/) environment or another Python environment in which you've installed the [Jupyter package](https://pypi.org/project/jupyter/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Prepare ONNX Model from HuggingFace MobileBert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers.onnx import FeaturesManager\n",
    "from pathlib import Path\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original model: https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_onnx_model_from_huggingface(hf_model_name, onnx_model_path):\n",
    "    \"\"\"\n",
    "        Load the model from huggingface and export it to onnx\n",
    "    \"\"\"\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(hf_model_name)\n",
    "    model = transformers.AutoModelForSequenceClassification.from_pretrained(hf_model_name)\n",
    "    \n",
    "    model_kind, model_onnx_config = FeaturesManager.check_supported_model_or_raise(model, feature=\"sequence-classification\")\n",
    "    onnx_config = model_onnx_config(model.config)\n",
    "    \n",
    "    # Export the hf model to onnx\n",
    "    onnx_inputs, onnx_outputs = transformers.onnx.export(tokenizer, # pretrained generic tokenizer class for the model\n",
    "                                                         model, # pretrained hf model\n",
    "                                                         onnx_config, # onnx configurations which includes input/output names/types info\n",
    "                                                         16, # opset_version - the ONNX version to export the model to\n",
    "                                                         onnx_model_path) # where to save the exported onnx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ONNX model from huggingface model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:218: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask, torch.tensor(torch.finfo(scores.dtype).min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "onnx_model_path = Path('distilbert-base-uncased-finetuned-sst-2-english.onnx')\n",
    "if not onnx_model_path.exists():\n",
    "    print(\"Creating ONNX model from huggingface model...\")\n",
    "    create_onnx_model_from_huggingface('distilbert-base-uncased-finetuned-sst-2-english', onnx_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the output ONNX model is exported successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert onnx_model_path.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantize the output model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model(model_path: Path):\n",
    "    \"\"\"\n",
    "        Quantize the model, so that it can be run on mobile devices with smaller memory footprint\n",
    "    \"\"\"\n",
    "    quantized_model_path = model_path.with_name(model_path.stem+\"_quant\").with_suffix(model_path.suffix)\n",
    "    quantize_dynamic(model_path, quantized_model_path, weight_type=QuantType.QInt8)\n",
    "    model_path.unlink()\n",
    "    return quantized_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.0/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.0/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.1/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.1/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.2/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.2/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.3/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.3/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.4/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.4/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.5/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.5/attention/MatMul_1]\n"
     ]
    }
   ],
   "source": [
    "quantized_model = quantize_model(onnx_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2. Add pre and post processing steps to ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime_extensions.tools.pre_post_processing import *\n",
    "from onnxruntime_extensions.tools import add_pre_post_processing_to_model as add_ppp\n",
    "from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pre_post_processing(input_model_path: Path, output_model_path: str, model_name: str = \"distilbert-base-uncased-finetuned-sst-2-english\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    Add pre and post processing to the model, for tokenization and post processing\n",
    "    \"\"\"\n",
    "    onnx_opset = 16\n",
    "    model = onnx.load(str(input_model_path.resolve(strict=True)))\n",
    "    inputs = [create_named_value(\"input_text\", onnx.TensorProto.STRING, [1, \"num_sentences\"])]  # Fix the batch size to be 1\n",
    "    \n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    @contextmanager\n",
    "    def temp_vocab_file():\n",
    "        vocab_file = input_model_path.parent/ \"vocab.txt\"\n",
    "        yield vocab_file\n",
    "\n",
    "    with temp_vocab_file() as vocab_file:\n",
    "        import json\n",
    "        with open(str(vocab_file), 'w') as f:\n",
    "            f.write(json.dumps(tokenizer.vocab))\n",
    "\n",
    "        pipeline = PrePostProcessor(inputs, onnx_opset)\n",
    "        \n",
    "        tokenizer_args = TokenizerParam(\n",
    "            vocab_or_file=vocab_file,\n",
    "            do_lower_case=True,\n",
    "        )\n",
    "        \n",
    "        pipeline.add_pre_processing(\n",
    "            [\n",
    "                BertTokenizer(tokenizer_args),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        pipeline.add_post_processing(\n",
    "            [\n",
    "                ArgMax(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    new_model = pipeline.run(model)\n",
    "    onnx.save_model(new_model, output_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_path = str(quantized_model).replace(\".onnx\", \"_with_pre_post_processing.onnx\")\n",
    "add_pre_post_processing(quantized_model, output_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Test output ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime_extensions import get_library_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_onnx_model(model_path: str):\n",
    "    \n",
    "    so = ort.SessionOptions()\n",
    "    so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "\n",
    "    # Note: register the custom operators for the image decode/encode pre/post processing provided by onnxruntime-extensions\n",
    "    # with onnxruntime. if we do not do this we'll get an error on model load about the operators not being found.\n",
    "    ortext_lib_path = get_library_path()\n",
    "    so.register_custom_ops_library(ortext_lib_path)\n",
    "    inference_session = ort.InferenceSession(model_path, so)\n",
    "    \n",
    "    text = 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.';\n",
    "    text_2 = 'I love transformers!'\n",
    "    outputs = inference_session.run(['index'], {'input_text': [[text]]})\n",
    "    output_answer = outputs[0]\n",
    "    print(\"\\nInput text: \" + text)\n",
    "    print(\"\\nResult is: \" + (\"POSITIVE\" if output_answer[0] == 1 else \"NEGATIVE\"))\n",
    "    \n",
    "    print(\"\\nInput text: \" + text_2)\n",
    "    outputs = inference_session.run(['index'], {'input_text': [[text_2]]})\n",
    "    output_answer = outputs[0]\n",
    "    print(\"\\nResult is: \" + (\"POSITIVE\" if output_answer[0] == 1 else \"NEGATIVE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input text: Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.\n",
      "\n",
      "Result is: NEGATIVE\n",
      "\n",
      "Input text: I love transformers!\n",
      "\n",
      "Result is: POSITIVE\n"
     ]
    }
   ],
   "source": [
    "test_onnx_model(output_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Build and run inference with the output model in a mobile application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can plug in the generated onnx model and run inference easily in a mobile application (Android/iOS) with the pre/post processing support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Android**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example code snippet for initalizing ort session and register ort extensions for pre/post processing support:\n",
    "```kotlin\n",
    "\n",
    "   // Initialize Ort Session and register the onnxruntime extensions package that contains the custom operators.\n",
    "    val sessionOptions: OrtSession.SessionOptions = OrtSession.SessionOptions()\n",
    "    if (ep.contains(\"CPU\")){\n",
    "    } else if (ep.contains(\"NNAPI\")) {\n",
    "        sessionOptions.addNnapi()\n",
    "    } else if (ep.contains(\"XNNAPCK\")) {\n",
    "        val po = mapOf<String, String>()\n",
    "        sessionOptions.addXnnpack(po)\n",
    "    }\n",
    "    sessionOptions.setSessionLogLevel(OrtLoggingLevel.ORT_LOGGING_LEVEL_VERBOSE)\n",
    "    sessionOptions.setSessionLogVerbosityLevel(0)\n",
    "    sessionOptions.registerCustomOpLibrary(OrtxPackage.getLibraryPath())\n",
    "    ortSession = ortEnv.createSession(readModel(), sessionOptions)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example code snippet for performing Question Answering task with MobileBERT model in an Android app:\n",
    "```kotlin\n",
    "\n",
    "    // Step 1: Prepare the input text\n",
    "    val text = text_seq.toString()\n",
    "\n",
    "    // Step 2: Create input tensor\n",
    "    val shape = longArrayOf(1, 1)\n",
    "    val inputTensor = OnnxTensor.createTensor(ortEnv, arrayOf(text), shape)\n",
    "\n",
    "    inputTensor.use {\n",
    "        // Step 3: Call Ort InferenceSession Run\n",
    "        val output = ortSession.run(Collections.singletonMap(\"input_text\", inputTensor))\n",
    "\n",
    "        // Step 4: Analyze the inference result\n",
    "        output.use {\n",
    "            val rawOutput = (output?.get(0)?.value) as Array<String>\n",
    "            // POSITIVE if 1 else NEGATIVE\n",
    "            outputAnswer = rawOutput[0]\n",
    "        }\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **iOS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example code snippet for performing Question Answering task in an iOS app:\n",
    "\n",
    "\n",
    "```c\n",
    "    // Step 1: Register custom ops\n",
    "    const auto ort_log_level = ORT_LOGGING_LEVEL_INFO;\n",
    "    auto ort_env = Ort::Env(ort_log_level, \"ORTObjectDetection\");\n",
    "    auto session_options = Ort::SessionOptions(); \n",
    "    \n",
    "    if (RegisterCustomOps(session_options, OrtGetApiBase()) != nullptr) {\n",
    "        throw std::runtime_error(\"RegisterCustomOps failed\");\n",
    "    }\n",
    "            \n",
    "    // Step 2: Load model   \n",
    "    NSString *model_path = [NSBundle.mainBundle pathForResource:@\"yolov8n_with_pre_post_processing\"\n",
    "                                                                ofType:@\"onnx\"];\n",
    "    if (model_path == nullptr) {\n",
    "        throw std::runtime_error(\"Failed to get model path\");\n",
    "    }\n",
    "            \n",
    "    // Step 2: Create Ort Inference Session    \n",
    "    auto sess = Ort::Session(ort_env, [model_path UTF8String], session_options);\n",
    "            \n",
    "    // Step 3: Prepare input tensors and input/output names        \n",
    "    std::vector<int64_t> input_dims{1, 1};\n",
    "    Ort::AllocatorWithDefaultOptions ortAllocator;\n",
    "    auto input_tensor = Ort::Value::CreateTensor(ortAllocator, input_dims.data(), input_dims.size(), ONNX_TENSOR_ELEMENT_DATA_TYPE_STRING);\n",
    "P\n",
    "    std::vector<std::string> input_vec;\n",
    "    input_vec.reserve(1);\n",
    "    input_vec.push_back([input_user_text UTF8String]);\n",
    "    std::vector<const char*> p_str;\n",
    "    for (const auto& s : input_vec) {\n",
    "        p_str.push_back(s.c_str());\n",
    "    }\n",
    "    input_tensor.FillStringTensor(p_str.data(), p_str.size());\n",
    " \n",
    "    // Step 4: Call inference session run\n",
    "    constexpr auto input_names = std::array{\"input_text\"};\n",
    "    constexpr auto output_names = std::array{\"index\"};\n",
    "    const auto outputs = sess.Run(Ort::RunOptions(), input_names.data(),\n",
    "                                &input_tensor, 1, output_names.data(), 1);\n",
    "                    \n",
    "    // Step 5: Analyze model outputs\n",
    "    if (outputs.size() != 1) {\n",
    "        throw std::runtime_error(\"Unexpected number of outputs\");\n",
    "    }        \n",
    "    const auto &output_tensor = outputs.at(1);\n",
    "    const std::string* output_string_raw = output_tensor.GetTensorData<int64_t>();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Model input/output names, sizes, types may require corresponding adjustment according to specific models using. The above act as sample code for demonstration usage purpose."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
