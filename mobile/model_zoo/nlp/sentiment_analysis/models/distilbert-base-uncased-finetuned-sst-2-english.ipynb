{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX Runtime Sentiment Analysis with MobileBert model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you will learn the end-to-end steps from obtaining a HuggingFace model, converting to ONNX format, adding pre/post processing steps to the ONNX model using onnxruntime-extensions library and finally plug in and apply inference in a sample mobile android/ios app if applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to pip install `onnxruntime onnx onnxruntime_extensions transformers` as the necessary libraries.\n",
    "\n",
    "```\n",
    "    pip install onnx onnxruntime onnxruntime_extensions\n",
    "```\n",
    "```\n",
    "    pip install transformers\n",
    "```\n",
    "\n",
    "To work with Python in Jupyter Notebooks, you must activate an [Anaconda](https://www.anaconda.com/) environment or another Python environment in which you've installed the [Jupyter package](https://pypi.org/project/jupyter/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Prepare ONNX Model from HuggingFace MobileBert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers.onnx import FeaturesManager\n",
    "from pathlib import Path\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original model: https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_onnx_model_from_huggingface(hf_model_name, onnx_model_path):\n",
    "    \"\"\"\n",
    "        Load the model from huggingface and export it to onnx\n",
    "    \"\"\"\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(hf_model_name)\n",
    "    model = transformers.AutoModelForSequenceClassification.from_pretrained(hf_model_name)\n",
    "    \n",
    "    model_kind, model_onnx_config = FeaturesManager.check_supported_model_or_raise(model, feature=\"sequence-classification\")\n",
    "    onnx_config = model_onnx_config(model.config)\n",
    "    \n",
    "    # Export the hf model to onnx\n",
    "    onnx_inputs, onnx_outputs = transformers.onnx.export(tokenizer, # pretrained generic tokenizer class for the model\n",
    "                                                         model, # pretrained hf model\n",
    "                                                         onnx_config, # onnx configurations which includes input/output names/types info\n",
    "                                                         16, # opset_version - the ONNX version to export the model to\n",
    "                                                         onnx_model_path) # where to save the exported onnx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ONNX model from huggingface model...\n",
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "onnx_model_path = Path('mobilebert-uncased-mnli.onnx')\n",
    "if not onnx_model_path.exists():\n",
    "    print(\"Creating ONNX model from huggingface model...\")\n",
    "    create_onnx_model_from_huggingface('typeform/mobilebert-uncased-mnli', onnx_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the output ONNX model is exported successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert onnx_model_path.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantize the output model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model(model_path: Path):\n",
    "    \"\"\"\n",
    "        Quantize the model, so that it can be run on mobile devices with smaller memory footprint\n",
    "    \"\"\"\n",
    "    quantized_model_path = model_path.with_name(model_path.stem+\"_quant\").with_suffix(model_path.suffix)\n",
    "    quantize_dynamic(model_path, quantized_model_path, weight_type=QuantType.QInt8)\n",
    "    model_path.unlink()\n",
    "    return quantized_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.0/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.0/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.1/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.1/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.2/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.2/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.3/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.3/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.4/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.4/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.5/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.5/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.6/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.6/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.7/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.7/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.8/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.8/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.9/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.9/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.10/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.10/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.11/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.11/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.12/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.12/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.13/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.13/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.14/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.14/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.15/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.15/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.16/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.16/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.17/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.17/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.18/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.18/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.19/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.19/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.20/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.20/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.21/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.21/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.22/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.22/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.23/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.23/attention/self/MatMul_1]\n"
     ]
    }
   ],
   "source": [
    "quantized_model = quantize_model(onnx_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2. Add pre and post processing steps to ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime_extensions.tools.pre_post_processing import *\n",
    "from onnxruntime_extensions.tools import add_pre_post_processing_to_model as add_ppp\n",
    "from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pre_post_processing(input_model_path: Path, output_model_path: str, model_name: str = \"typeform/mobilebert-uncased-mnli\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    Add pre and post processing to the model, for tokenization and post processing\n",
    "    \"\"\"\n",
    "    onnx_opset = 16\n",
    "    model = onnx.load(str(input_model_path.resolve(strict=True)))\n",
    "    inputs = [create_named_value(\"input_text\", onnx.TensorProto.STRING, [1, \"num_sentences\"])]  # Fix the batch size to be 1\n",
    "    \n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    @contextmanager\n",
    "    def temp_vocab_file():\n",
    "        vocab_file = input_model_path.parent/ \"vocab.txt\"\n",
    "        yield vocab_file\n",
    "\n",
    "    with temp_vocab_file() as vocab_file:\n",
    "        import json\n",
    "        with open(str(vocab_file), 'w') as f:\n",
    "            f.write(json.dumps(tokenizer.vocab))\n",
    "\n",
    "        pipeline = PrePostProcessor(inputs, onnx_opset)\n",
    "        \n",
    "        tokenizer_args = TokenizerParam(\n",
    "            vocab_or_file=vocab_file,\n",
    "            do_lower_case=True,\n",
    "        )\n",
    "        \n",
    "        pipeline.add_pre_processing(\n",
    "            [\n",
    "                BertTokenizer(tokenizer_args),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        pipeline.add_post_processing(\n",
    "            [\n",
    "                ArgMax(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    new_model = pipeline.run(model)\n",
    "    onnx.save_model(new_model, output_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_path = str(quantized_model).replace(\".onnx\", \"_with_pre_post_processing.onnx\")\n",
    "add_pre_post_processing(quantized_model, output_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Test output ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime_extensions import get_library_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_onnx_model(model_path: str):\n",
    "    \n",
    "    so = ort.SessionOptions()\n",
    "    so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "\n",
    "    # Note: register the custom operators for the image decode/encode pre/post processing provided by onnxruntime-extensions\n",
    "    # with onnxruntime. if we do not do this we'll get an error on model load about the operators not being found.\n",
    "    ortext_lib_path = get_library_path()\n",
    "    so.register_custom_ops_library(ortext_lib_path)\n",
    "    inference_session = ort.InferenceSession(model_path, so)\n",
    "    \n",
    "    text = 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.';\n",
    "    text_2 = 'I love transformers!'\n",
    "    outputs = inference_session.run(['index'], {'input_text': [[text]]})\n",
    "    output_answer = outputs[0]\n",
    "    print(\"\\nInput text: \" + text)\n",
    "    print(\"\\nResult is: \" + (\"POSITIVE\" if output_answer[0] == 1 else \"NEGATIVE\"))\n",
    "    \n",
    "    print(\"\\nInput text: \" + text_2)\n",
    "    outputs = inference_session.run(['index'], {'input_text': [[text_2]]})\n",
    "    output_answer = outputs[0]\n",
    "    print(\"\\nResult is: \" + (\"POSITIVE\" if output_answer[0] == 1 else \"NEGATIVE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input text: Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.\n",
      "\n",
      "Result is: POSITIVE\n",
      "\n",
      "Input text: I love transformers!\n",
      "\n",
      "Result is: POSITIVE\n"
     ]
    }
   ],
   "source": [
    "test_onnx_model(output_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Build and run inference with the output model in a mobile application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can plug in the generated onnx model and run inference easily in a mobile application (Android/iOS) with the pre/post processing support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Android**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example code snippet for initalizing ort session and register ort extensions for pre/post processing support:\n",
    "```kotlin\n",
    "\n",
    "   // Initialize Ort Session and register the onnxruntime extensions package that contains the custom operators.\n",
    "    val sessionOptions: OrtSession.SessionOptions = OrtSession.SessionOptions()\n",
    "    if (ep.contains(\"CPU\")){\n",
    "    } else if (ep.contains(\"NNAPI\")) {\n",
    "        sessionOptions.addNnapi()\n",
    "    } else if (ep.contains(\"XNNAPCK\")) {\n",
    "        val po = mapOf<String, String>()\n",
    "        sessionOptions.addXnnpack(po)\n",
    "    }\n",
    "    sessionOptions.setSessionLogLevel(OrtLoggingLevel.ORT_LOGGING_LEVEL_VERBOSE)\n",
    "    sessionOptions.setSessionLogVerbosityLevel(0)\n",
    "    sessionOptions.registerCustomOpLibrary(OrtxPackage.getLibraryPath())\n",
    "    ortSession = ortEnv.createSession(readModel(), sessionOptions)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example code snippet for performing Question Answering task with MobileBERT model in an Android app:\n",
    "```kotlin\n",
    "\n",
    "    // Step 1: Prepare the input text\n",
    "    val text = text_seq.toString()\n",
    "\n",
    "    // Step 2: Create input tensor\n",
    "    val shape = longArrayOf(1, 1)\n",
    "    val inputTensor = OnnxTensor.createTensor(ortEnv, arrayOf(text), shape)\n",
    "\n",
    "    inputTensor.use {\n",
    "        // Step 3: Call Ort InferenceSession Run\n",
    "        val output = ortSession.run(Collections.singletonMap(\"input_text\", inputTensor))\n",
    "\n",
    "        // Step 4: Analyze the inference result\n",
    "        output.use {\n",
    "            val rawOutput = (output?.get(0)?.value) as Array<String>\n",
    "            // POSITIVE if 1 else NEGATIVE\n",
    "            outputAnswer = rawOutput[0]\n",
    "        }\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **iOS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example code snippet for performing Question Answering task in an iOS app:\n",
    "\n",
    "\n",
    "```c\n",
    "    // Step 1: Register custom ops\n",
    "    const auto ort_log_level = ORT_LOGGING_LEVEL_INFO;\n",
    "    auto ort_env = Ort::Env(ort_log_level, \"ORTObjectDetection\");\n",
    "    auto session_options = Ort::SessionOptions(); \n",
    "    \n",
    "    if (RegisterCustomOps(session_options, OrtGetApiBase()) != nullptr) {\n",
    "        throw std::runtime_error(\"RegisterCustomOps failed\");\n",
    "    }\n",
    "            \n",
    "    // Step 2: Load model   \n",
    "    NSString *model_path = [NSBundle.mainBundle pathForResource:@\"yolov8n_with_pre_post_processing\"\n",
    "                                                                ofType:@\"onnx\"];\n",
    "    if (model_path == nullptr) {\n",
    "        throw std::runtime_error(\"Failed to get model path\");\n",
    "    }\n",
    "            \n",
    "    // Step 2: Create Ort Inference Session    \n",
    "    auto sess = Ort::Session(ort_env, [model_path UTF8String], session_options);\n",
    "            \n",
    "    // Step 3: Prepare input tensors and input/output names        \n",
    "    std::vector<int64_t> input_dims{1, 1};\n",
    "    Ort::AllocatorWithDefaultOptions ortAllocator;\n",
    "    auto input_tensor = Ort::Value::CreateTensor(ortAllocator, input_dims.data(), input_dims.size(), ONNX_TENSOR_ELEMENT_DATA_TYPE_STRING);\n",
    "P\n",
    "    std::vector<std::string> input_vec;\n",
    "    input_vec.reserve(1);\n",
    "    input_vec.push_back([input_user_text UTF8String]);\n",
    "    std::vector<const char*> p_str;\n",
    "    for (const auto& s : input_vec) {\n",
    "        p_str.push_back(s.c_str());\n",
    "    }\n",
    "    input_tensor.FillStringTensor(p_str.data(), p_str.size());\n",
    " \n",
    "    // Step 4: Call inference session run\n",
    "    constexpr auto input_names = std::array{\"input_text\"};\n",
    "    constexpr auto output_names = std::array{\"index\"};\n",
    "    const auto outputs = sess.Run(Ort::RunOptions(), input_names.data(),\n",
    "                                &input_tensor, 1, output_names.data(), 1);\n",
    "                    \n",
    "    // Step 5: Analyze model outputs\n",
    "    if (outputs.size() != 1) {\n",
    "        throw std::runtime_error(\"Unexpected number of outputs\");\n",
    "    }        \n",
    "    const auto &output_tensor = outputs.at(1);\n",
    "    const std::string* output_string_raw = output_tensor.GetTensorData<int64_t>();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Model input/output names, sizes, types may require corresponding adjustment according to specific models using. The above act as sample code for demonstration usage purpose."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
